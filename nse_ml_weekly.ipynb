{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic variables\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-12-31' \n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_tickers = pd.read_csv(\"C:/Users/satya/Downloads/ind_nifty500list (1).csv\")['Symbol']\n",
    "nse_tickers = nse_tickers.to_list()\n",
    "for count in range(len(nse_tickers)):\n",
    "    nse_tickers[count] = nse_tickers[count] + \".NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = yf.download(nse_tickers , start_date, end_date)\n",
    "price_data = price_data['Adj Close']\n",
    "price_data = price_data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_stocks_data = clean_data(price_data, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_stocks_data = price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df = calculate_returns(nse_stocks_data, rebalance='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:/Users/satya/OneDrive/Desktop/model_outputs/'\n",
    "filename = 'returns_df_nse_weekly.csv'\n",
    "\n",
    "# Save the DataFrame to the specified folder\n",
    "returns_df.to_csv(folder_path + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_df = calculate_12_1_momentum(nse_stocks_data, resample='W')\n",
    "momentum_reversion = calculate_m_momentum(nse_stocks_data, 1, resample='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_reversion.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_3 = calculate_return_signals(returns_df, 3, rebalance='W')\n",
    "seasonality_5 = calculate_return_signals(returns_df, 5, rebalance='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df = calculate_vol(returns_df, 1, resample='W')\n",
    "vol_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_3 = convert_signals(seasonality_3, 2)\n",
    "seasonality_5 = convert_signals(seasonality_5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_returns = convert_df_values(returns_df, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_3 = seasonality_3.rename_axis('Ticker')\n",
    "seasonality_5 = seasonality_5.rename_axis('Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt each DataFrame\n",
    "bin_returns_melted = melt_df(bin_returns, 'bin_Return')\n",
    "returns_melted = melt_df(returns_df, 'Return')\n",
    "momentum_reversal_melted = melt_df(momentum_reversion, 'Momentum_Reversal')\n",
    "momentum_12_1_melted = melt_df(momentum_df, 'Momentum_12_1')\n",
    "vol_melted = melt_df(vol_df, 'Volatility')\n",
    "seasonality_3_melted = melt_df(seasonality_3, 'Seasonality_3')\n",
    "seasonality_5_melted = melt_df(seasonality_5, 'Seasonality_5')\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = bin_returns_melted\n",
    "merged_df = merged_df.merge(returns_melted, on=['Stock', 'Date'])\n",
    "merged_df = merged_df.merge(momentum_reversal_melted, on=['Stock', 'Date'])\n",
    "merged_df = merged_df.merge(momentum_12_1_melted, on=['Stock', 'Date'])\n",
    "merged_df = merged_df.merge(vol_melted, on=['Stock', 'Date'])\n",
    "merged_df = merged_df.merge(seasonality_3_melted, on=['Stock', 'Date'])\n",
    "merged_df = merged_df.merge(seasonality_5_melted, on=['Stock', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "\n",
    "# Filter rows where the date is greater than 2005-12-31\n",
    "filtered_df = merged_df[merged_df['Date'] > '2005-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(['Stock'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtered_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the rows with all 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.bin_Return != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data.Date > '2005-12-31'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "\n",
    "num_cols = df.select_dtypes(include = ['float64']).columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "\n",
    "cat_cols = df.select_dtypes(include = ['int64']).columns\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color mapping for the categories\n",
    "color_map = {1: 'blue', 0: 'green', -1: 'red'}\n",
    "colors = [color_map[1], color_map[0], color_map[-1]]\n",
    "\n",
    "# Initialize the plot\n",
    "count = 1\n",
    "fig, axs = plt.subplots(1, len(cat_cols), figsize=(20, 10))  # Increase the overall figure size\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "for ax, col in zip(axs, cat_cols):\n",
    "    # Calculate value counts and percentages\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentages = value_counts / value_counts.sum() * 100\n",
    "    \n",
    "    # Plot pie chart with consistent colors\n",
    "    wedges, texts, autotexts = ax.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, colors=[color_map[key] for key in percentages.index], textprops=dict(color=\"w\", fontsize=14, fontweight='bold'))\n",
    "    \n",
    "    ax.set_title(f'Pie chart of {col}', fontsize=16)  # Increase title font size\n",
    "    \n",
    "    # Set properties for percentage labels\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(14)\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# Create a legend on the bottom right\n",
    "labels = ['1', '0', '-1']\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[int(label)], markersize=10) for label in labels]\n",
    "fig.legend(handles, labels, title='Categories', loc='lower right', fontsize=14, title_fontsize='16')\n",
    "\n",
    "plt.suptitle('Pie charts of categorical variables', size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize=(16,8)) # Create a new figure for each plot with a specified size\n",
    "    plt.subplot(2,2,1)\n",
    "    sns.distplot(df[col])\n",
    "    plt.subplot(2,2,2)\n",
    "    sns.boxplot(data=df, y=col)\n",
    "    plt.ylabel(col)\n",
    "    plt.title(f'Box Plot of {col}', fontsize=16)\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Outlier using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['Date'], axis=1)\n",
    "# Define z-score threshold\n",
    "z_threshold = threshold\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "z_scores = df1.apply(zscore)\n",
    "\n",
    "# Filter rows where any z-score is greater than the threshold\n",
    "df_filtered = df1[(z_scores.abs() <= z_threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_filtered, df, on=['bin_Return', 'Return', 'Momentum_Reversal', 'Momentum_12_1', 'Volatility',\n",
    "                                   'Seasonality_3', 'Seasonality_5'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color mapping for the categories\n",
    "color_map = {1: 'blue', 0: 'green', -1: 'red'}\n",
    "colors = [color_map[1], color_map[0], color_map[-1]]\n",
    "\n",
    "# Initialize the plot\n",
    "count = 1\n",
    "fig, axs = plt.subplots(1, len(cat_cols), figsize=(20, 10))  # Increase the overall figure size\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "for ax, col in zip(axs, cat_cols):\n",
    "    # Calculate value counts and percentages\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentages = value_counts / value_counts.sum() * 100\n",
    "    \n",
    "    # Plot pie chart with consistent colors\n",
    "    wedges, texts, autotexts = ax.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, colors=[color_map[key] for key in percentages.index], textprops=dict(color=\"w\", fontsize=14, fontweight='bold'))\n",
    "    \n",
    "    ax.set_title(f'Pie chart of {col}', fontsize=16)  # Increase title font size\n",
    "    \n",
    "    # Set properties for percentage labels\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(14)\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# Create a legend on the bottom right\n",
    "labels = ['1', '0', '-1']\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[int(label)], markersize=10) for label in labels]\n",
    "fig.legend(handles, labels, title='Categories', loc='lower right', fontsize=14, title_fontsize='16')\n",
    "\n",
    "plt.suptitle('Pie charts of categorical variables', size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize=(16,8)) # Create a new figure for each plot with a specified size\n",
    "    plt.subplot(2,2,1)\n",
    "    sns.distplot(df[col])\n",
    "    plt.subplot(2,2,2)\n",
    "    sns.boxplot(data=df, y=col)\n",
    "    plt.ylabel(col)\n",
    "    plt.title(f'Box Plot of {col}', fontsize=16)\n",
    "    plt.grid(False)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols[1:]:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(data = df, x= col, y= df_filtered.Return)\n",
    "    plt.title(f'Scatter Plot of {col} vs Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df_filtered.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Merge the DataFrames based on index\n",
    "# merged_df = df_filtered.merge(filtered_df[['Date']], left_index=True, right_index=True, how='left', suffixes=('', '_filtered'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2018-12-31'\n",
    "\n",
    "# Split into training and testing data based on date\n",
    "train_df = df[df['Date'] <= split_date].reset_index(drop=True)\n",
    "test_df = df[df['Date'] > split_date].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target for training data\n",
    "X_train = train_df.drop(columns=['Return', 'Date', 'bin_Return'])\n",
    "y_train = train_df['bin_Return']\n",
    "y_train_regn = train_df['Return']\n",
    "\n",
    "# Features and target for testing data\n",
    "X_test = test_df.drop(columns=['Return', 'Date', 'bin_Return'])\n",
    "y_test = test_df['bin_Return']\n",
    "y_test_regn = test_df['Return']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "dt = DecisionTreeClassifier(max_depth=50, random_state=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the testing data\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid to search through\n",
    "# param_grid = {\n",
    "#     'max_depth': [None, 10, 20, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV to find the best parameters\n",
    "# grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data0\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and the best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = DecisionTreeClassifier(max_depth = 10, max_features = 'sqrt', min_samples_split = 2, random_state=75, max_leaf_nodes=4)\n",
    "best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = best.predict(X_test)\n",
    "k = best.predict(X_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred_gs)\n",
    "a = accuracy_score(y_train, k)\n",
    "print(\"\\nAccuracy:\", accuracy, 'train', a)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [20]:\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=600, random_state=90, max_depth=10, max_features = 'log2', max_leaf_nodes=8)\n",
    "\n",
    "    # Fit the Random Forest classifier on the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    k_pred = rf_classifier.predict(X_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    a = accuracy_score(y_train, k_pred)\n",
    "    # print(f'n_estimators:{n}')\n",
    "    print(\"Accuracy:\", accuracy, a)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_names = X_train.columns\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "for i,index in enumerate(sorted_indices):\n",
    "    print(f\"{i+1}. {feature_names[index]}: {feature_importance[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10, random_state=75)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "k_pred = log_reg.predict(X_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred_log)\n",
    "a = accuracy_score(y_train, k_pred)\n",
    "print(\"Accuracy:\", accuracy, 'train:', a)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base classifiers\n",
    "base_classifiers = [\n",
    "    ('decision_tree', DecisionTreeClassifier( random_state=75)),\n",
    "    ('log_reg', LogisticRegression(max_iter=50, random_state=75)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=500, random_state=75))\n",
    "]\n",
    "\n",
    "# Define the meta-learner\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "# Create the StackingClassifier\n",
    "stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# Fit the StackingClassifier on the training data\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_ada = stacking_clf.predict(X_test)\n",
    "# Predict on the testing data\n",
    "k_train = stacking_clf.predict(X_train)\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "a = accuracy_score(y_train, k_train)\n",
    "print(\"Accuracy:\", accuracy, a)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid for classification\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(128, 64), (32, 16), (64, 32)],\n",
    "#     'activation': ['relu', 'tanh', 'logistic'],\n",
    "#     'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "#     'max_iter': [200, 500],\n",
    "# }\n",
    "\n",
    "# # Initialize the MLP Classifier model\n",
    "# mlp = MLPClassifier(random_state=75)\n",
    "\n",
    "# # Initialize the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', verbose=2)\n",
    "\n",
    "# # Fit the grid search on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(f\"Best Parameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from grid search\n",
    "best_mlp_clf = MLPClassifier(activation='logistic', hidden_layer_sizes=(50, 10), learning_rate_init=0.01, max_iter=200, random_state=75)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the best model\n",
    "y_pred_nn_clf = best_mlp_clf.predict(X_test)\n",
    "y_pred_train = best_mlp_clf.predict(X_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_nn_clf)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "classification_report_test = classification_report(y_test, y_pred_nn_clf)\n",
    "classification_report_train = classification_report(y_train, y_pred_train)\n",
    "\n",
    "# Print accuracy up to 2 decimal places\n",
    "print(f\"Train Accuracy: {accuracy_train:.2f}, Test Accuracy: {accuracy_test:.2f}\")\n",
    "\n",
    "# Print classification reports\n",
    "print(\"\\nTrain Classification Report:\\n\", classification_report_train)\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cont models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    normalized_series = (series - mean) / std\n",
    "    return normalized_series\n",
    "\n",
    "def normalize_series_using_train_stats(train_series, test_series):\n",
    "    mean = train_series.mean()\n",
    "    std = train_series.std()\n",
    "    normalized_test_series = (test_series - mean) / std\n",
    "    return normalized_test_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df, columns):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        df[col] = (df[col] - mean) / std\n",
    "    return df\n",
    "\n",
    "def normalize_columns_using_train_stats(train_df, test_df, columns):\n",
    "    for col in columns:\n",
    "        mean = train_df[col].mean()\n",
    "        std = train_df[col].std()\n",
    "        test_df[col] = (test_df[col] - mean) / std\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_x, mean_sd_x = [], []\n",
    "for col in num_cols[1:]:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "\n",
    "mean_train_y = y_train_regn.mean()\n",
    "sd_train_y = y_train_regn.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized = normalize_columns_using_train_stats(X_train, X_test, num_cols[1:])\n",
    "X_train_normalized = normalize_columns(X_train, num_cols[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(X_train_normalized.Momentum_Reversal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_regn = normalize_series_using_train_stats(y_train_regn, y_test_regn)\n",
    "y_train_regn = normalize_series(y_train_regn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(y_train_regn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear Regression model\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "regression_model.fit(X_train_normalized, y_train_regn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the testing data\n",
    "y_cont_pred = regression_model.predict(X_test_normalized)\n",
    "k_pred = regression_model.predict(X_train_normalized)\n",
    "\n",
    "# Evaluate the regressor on training data\n",
    "train_mse = mean_squared_error(y_train_regn, k_pred)\n",
    "train_r2 = r2_score(y_train_regn, k_pred)\n",
    "train_adjusted_r2 = 1 - (1 - train_r2) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_normalized.shape[1] - 1)\n",
    "\n",
    "\n",
    "# Evaluate the regressor on testing data\n",
    "test_mse = mean_squared_error(y_test_regn, y_cont_pred)\n",
    "test_r2 = r2_score(y_test_regn, y_cont_pred)\n",
    "test_adjusted_r2 = 1 - (1 - test_r2) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Training R2: {train_r2}\")\n",
    "print(f\"Testing R2: {test_r2}\")\n",
    "print(f\"Training Adjusted R2: {train_adjusted_r2}\")\n",
    "print(f\"Testing Adjusted R2: {test_adjusted_r2}\")\n",
    "y_cont_pred = (y_cont_pred*sd_train_y) + mean_train_y\n",
    "train_y_regn = (k_pred*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(k_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred are your actual and predicted values, respectively\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_regn, y_cont_pred, alpha=0.5)\n",
    "plt.title('Scatterplot of Predicted vs Actual Values')\n",
    "plt.xlabel('Actual Values (y_train)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "# plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Line y=x for reference\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for n in [10]:\n",
    "    # print(n)\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=10, random_state=20, max_depth=5, max_features = 'log2')\n",
    "\n",
    "    # Fit the Random Forest regressor on the training data\n",
    "    rf_regressor.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    y_pred_rf_regn = rf_regressor.predict(X_test_normalized)\n",
    "    k_pred = rf_regressor.predict(X_train_normalized)\n",
    "\n",
    "    # Evaluate the regressor on training data\n",
    "    train_mse = mean_squared_error(y_train_regn, k_pred)\n",
    "    train_r2 = r2_score(y_train_regn, k_pred)\n",
    "    train_adjusted_r2 = 1 - (1 - train_r2) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_normalized.shape[1] - 1)\n",
    "\n",
    "    print(f\"Training MSE: {train_mse}\")\n",
    "    print(f\"Training R2: {train_r2}\")\n",
    "    print(f\"Training Adjusted R2: {train_adjusted_r2}\")\n",
    "\n",
    "    # Evaluate the regressor on testing data\n",
    "    test_mse = mean_squared_error(y_test_regn, y_pred_rf_regn)\n",
    "    test_r2 = r2_score(y_test_regn, y_pred_rf_regn)\n",
    "    test_adjusted_r2 = 1 - (1 - test_r2) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_normalized.shape[1] - 1)\n",
    "\n",
    "    print(f\"Testing MSE: {test_mse}\")\n",
    "    print(f\"Testing R2: {test_r2}\")\n",
    "    print(f\"Testing Adjusted R2: {test_adjusted_r2}\")\n",
    "    y_pred_rf_regn = (y_pred_rf_regn*sd_train_y) + mean_train_y\n",
    "    train_y_rf = (k_pred*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'max_depth': [5, 10, 30, 80],\n",
    "#     'min_samples_split': [2, 8, 15, 20],\n",
    "#     'min_samples_leaf': [5, 10, 20],\n",
    "#     'criterion':['squared_error' ,'poisson'],\n",
    "#     'splitter':['best', 'random'],\n",
    "#     'max_features':['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Initialize the Decision Tree regressor\n",
    "# dt_regressor = DecisionTreeRegressor(random_state=20)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# print(f\"Best Parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cart = DecisionTreeRegressor(criterion='squared_error', max_depth=10, min_samples_leaf=5, min_samples_split=2, splitter='random', max_features='sqrt')\n",
    "best_cart.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_dt_regn = best_cart.predict(X_test_normalized)\n",
    "k_pred = best_cart.predict(X_train_normalized)\n",
    "\n",
    "# Evaluate the regressor on training data\n",
    "train_mse = mean_squared_error(y_train_regn, k_pred)\n",
    "train_r2 = r2_score(y_train_regn, k_pred)\n",
    "train_adjusted_r2 = 1 - (1 - train_r2) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Training R2: {train_r2}\")\n",
    "print(f\"Training Adjusted R2: {train_adjusted_r2}\")\n",
    "\n",
    "# Evaluate the regressor on testing data\n",
    "test_mse = mean_squared_error(y_test_regn, y_pred_dt_regn)\n",
    "test_r2 = r2_score(y_test_regn, y_pred_dt_regn)\n",
    "test_adjusted_r2 = 1 - (1 - test_r2) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Testing R2: {test_r2}\")\n",
    "print(f\"Testing Adjusted R2: {test_adjusted_r2}\")\n",
    "y_pred_dt_regn = (y_pred_dt_regn*sd_train_y) + mean_train_y\n",
    "train_y_cart = (k_pred*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Lasso Regression model\n",
    "lasso_model = Lasso(alpha=0.1)  # Adjust alpha for regularization strength\n",
    "\n",
    "# Fit the model on the training data\n",
    "lasso_model.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_lasso_pred = lasso_model.predict(X_test_normalized)\n",
    "k_pred = lasso_model.predict(X_train_normalized)\n",
    "\n",
    "# Evaluate the regressor on training data\n",
    "train_mse = mean_squared_error(y_train_regn, k_pred)\n",
    "train_r2 = r2_score(y_train_regn, k_pred)\n",
    "train_adjusted_r2 = 1 - (1 - train_r2) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Training R2: {train_r2}\")\n",
    "print(f\"Training Adjusted R2: {train_adjusted_r2}\")\n",
    "\n",
    "# Evaluate the regressor on testing data\n",
    "test_mse = mean_squared_error(y_test_regn, y_lasso_pred)\n",
    "test_r2 = r2_score(y_test_regn, y_lasso_pred)\n",
    "test_adjusted_r2 = 1 - (1 - test_r2) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Testing R2: {test_r2}\")\n",
    "print(f\"Testing Adjusted R2: {test_adjusted_r2}\")\n",
    "y_lasso_pred = (y_lasso_pred*sd_train_y) + mean_train_y\n",
    "train_y_lasso = (k_pred*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter for regularization strength\n",
    "\n",
    "# Fit the model on the training data\n",
    "ridge_model.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_ridge_pred = ridge_model.predict(X_test_normalized)\n",
    "k_pred = ridge_model.predict(X_train_normalized)\n",
    "\n",
    "# Evaluate the regressor on training data\n",
    "train_mse = mean_squared_error(y_train_regn, k_pred)\n",
    "train_r2 = r2_score(y_train_regn, k_pred)\n",
    "train_adjusted_r2 = 1 - (1 - train_r2) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Training R2: {train_r2}\")\n",
    "print(f\"Training Adjusted R2: {train_adjusted_r2}\")\n",
    "\n",
    "# Evaluate the regressor on testing data\n",
    "test_mse = mean_squared_error(y_test_regn, y_ridge_pred)\n",
    "test_r2 = r2_score(y_test_regn, y_ridge_pred)\n",
    "test_adjusted_r2 = 1 - (1 - test_r2) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_normalized.shape[1] - 1)\n",
    "\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Testing R2: {test_r2}\")\n",
    "print(f\"Testing Adjusted R2: {test_adjusted_r2}\")\n",
    "y_ridge_pred = (y_ridge_pred*sd_train_y) + mean_train_y\n",
    "train_y_ridge = (k_pred*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the neural network model\n",
    "# def create_nn_model(input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Create and train the nn model\n",
    "# nn_model = create_nn_model(X_train_scaled.shape[1])\n",
    "# nn_model.fit(X_train_scaled, y_train_regn, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# # Predict on the training data\n",
    "# y_train_pred_nn = nn_model.predict(X_train_scaled)\n",
    "\n",
    "# # Predict on the testing data\n",
    "# y_test_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# # Calculate MSE for the training data\n",
    "# train_mse_nn = mean_squared_error(y_train_regn, y_train_pred_nn)\n",
    "# # Calculate R² for the training data\n",
    "# train_r2_nn = r2_score(y_train_regn, y_train_pred_nn)\n",
    "# # Calculate adjusted R² for the training data\n",
    "# train_adjusted_r2_nn = 1 - (1 - train_r2_nn) * (len(y_train_regn) - 1) / (len(y_train_regn) - X_train_scaled.shape[1] - 1)\n",
    "\n",
    "# # Calculate MSE for the testing data\n",
    "# test_mse_nn = mean_squared_error(y_test_regn, y_test_pred_nn)\n",
    "# # Calculate R² for the testing data\n",
    "# test_r2_nn = r2_score(y_test_regn, y_test_pred_nn)\n",
    "# # Calculate adjusted R² for the testing data\n",
    "# test_adjusted_r2_nn = 1 - (1 - test_r2_nn) * (len(y_test_regn) - 1) / (len(y_test_regn) - X_test_scaled.shape[1] - 1)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Training MSE: {train_mse_nn}\")\n",
    "# print(f\"Training R²: {train_r2_nn}\")\n",
    "# print(f\"Training Adjusted R²: {train_adjusted_r2_nn}\")\n",
    "\n",
    "# print(f\"Testing MSE: {test_mse_nn}\")\n",
    "# print(f\"Testing R²: {test_r2_nn}\")\n",
    "# print(f\"Testing Adjusted R²: {test_adjusted_r2_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "# def adjusted_r2(r2, n, k):\n",
    "#     return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(128, 64)],# (100, 50), (64, 32)\n",
    "#     'activation': ['relu', 'tanh', 'logistic'],\n",
    "#     'solver': ['adam'], #, 'sgd', 'lbfgs'\n",
    "#     'learning_rate_init': [0.1],#0.001, 0.01, 0.1\n",
    "#     'max_iter': [500],#200, 500, 1000\n",
    "# }\n",
    "\n",
    "# # Initialize the MLP Regressor model\n",
    "# mlp = MLPRegressor(random_state=75)\n",
    "\n",
    "# # Initialize the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
    "\n",
    "# # Fit the grid search on the training data\n",
    "# grid_search.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(f\"Best Parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_mlp = mlp = MLPRegressor(hidden_layer_sizes=(64, 32), activation='logistic', solver='adam', learning_rate_init=0.1, max_iter=500, random_state=75)\n",
    "\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_mlp.fit(X_train_normalized, y_train_regn)\n",
    "\n",
    "# Predict on the testing data using the best model\n",
    "y_pred_nn = best_mlp.predict(X_test_normalized)\n",
    "y_pred_nn_train = best_mlp.predict(X_train_normalized)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_test = mean_squared_error(y_test_regn, y_pred_nn)\n",
    "mse_train = mean_squared_error(y_train_regn, y_pred_nn_train)\n",
    "\n",
    "r2_test = r2_score(y_test_regn, y_pred_nn)\n",
    "r2_train = r2_score(y_train, y_pred_nn_train)\n",
    "\n",
    "adj_r2_test = adjusted_r2(r2_test, len(y_test_regn), X_test.shape[1])\n",
    "adj_r2_train = adjusted_r2(r2_train, len(y_train), X_train.shape[1])\n",
    "\n",
    "print(f\"Train MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Train Adjusted R^2: {adj_r2_train}, Test Adjusted R^2: {adj_r2_test}\")\n",
    "y_pred_nn = (y_pred_nn*sd_train_y) + mean_train_y\n",
    "train_y_nn = (y_pred_nn_train*sd_train_y) + mean_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buy Sell on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df =  pd.merge(train_df, merged_df[['Date', 'Stock', 'Return']], \n",
    "                      on=['Date', 'Return'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = best.predict(X_train)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_dt_train = {}\n",
    "sell_dict_dt_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_dt_train:\n",
    "            buy_dict_dt_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_dt_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_dt_train:\n",
    "            sell_dict_dt_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_dt_train[date] = [stock]\n",
    "\n",
    "buy_sell_dt_train = portfolio_return(returns_df, buy_dict_dt_train, sell_dict_dt_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rf_classifier.predict(X_train)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_rf_train = {}\n",
    "sell_dict_rf_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_rf_train:\n",
    "            buy_dict_rf_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_rf_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_rf_train:\n",
    "            sell_dict_rf_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_rf_train[date] = [stock]\n",
    "\n",
    "buy_sell_rf_train = portfolio_return(returns_df, buy_dict_rf_train, sell_dict_rf_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stacking_clf.predict(X_train)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_ada_train = {}\n",
    "sell_dict_ada_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_ada_train:\n",
    "            buy_dict_ada_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_ada_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_ada_train:\n",
    "            sell_dict_ada_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_ada_train[date] = [stock]\n",
    "\n",
    "buy_sell_ada_train = portfolio_return(returns_df, buy_dict_ada_train, sell_dict_ada_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = log_reg.predict(X_train)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_log_train = {}\n",
    "sell_dict_log_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_log_train:\n",
    "            buy_dict_log_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_log_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_log_train:\n",
    "            sell_dict_log_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_log_train[date] = [stock]\n",
    "\n",
    "buy_sell_log_train = portfolio_return(returns_df, buy_dict_log_train, sell_dict_log_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_nn_clf_train = {}\n",
    "sell_dict_nn_clf_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on redictions\n",
    "for idx, pred in enumerate(y_pred_train):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_nn_clf_train:\n",
    "            buy_dict_nn_clf_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_nn_clf_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_nn_clf_train:\n",
    "            sell_dict_nn_clf_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_nn_clf_train[date] = [stock]\n",
    "\n",
    "buy_sell_nn_clf_train = portfolio_return(returns_df, buy_dict_nn_clf_train, sell_dict_nn_clf_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cont Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = regression_model.predict(X_train_normalized)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_train = {}\n",
    "sell_dict_regn_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_regn):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred >0:\n",
    "        if date in buy_dict_regn_train:\n",
    "            buy_dict_regn_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_regn_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_regn_train:\n",
    "            sell_dict_regn_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_regn_train[date] = [stock]\n",
    "\n",
    "buy_sell_regn_train = portfolio_return(returns_df, buy_dict_regn_train, sell_dict_regn_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rf Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_rf_train = {}\n",
    "sell_dict_regn_rf_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_rf):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred >0:\n",
    "        if date in buy_dict_regn_rf_train:\n",
    "            buy_dict_regn_rf_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_regn_rf_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_regn_rf_train:\n",
    "            sell_dict_regn_rf_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_regn_rf_train[date] = [stock]\n",
    "\n",
    "buy_sell_regn_rf_train = portfolio_return(returns_df, buy_dict_regn_rf_train, sell_dict_regn_rf_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_cart_train = {}\n",
    "sell_dict_cart_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_cart):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred >0:\n",
    "        if date in buy_dict_cart_train:\n",
    "            buy_dict_cart_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_cart_train[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_cart_train:\n",
    "            sell_dict_cart_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_cart_train[date] = [stock]\n",
    "\n",
    "buy_sell_cart_train = portfolio_return(returns_df, buy_dict_cart_train, sell_dict_cart_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_ridge_train = {}\n",
    "sell_dict_ridge_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_ridge):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "\n",
    "    if pred > 0:\n",
    "        if date in buy_dict_ridge_train:\n",
    "            buy_dict_ridge_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_ridge_train[date] = [stock]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_ridge_train:\n",
    "            sell_dict_ridge_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_ridge_train[date] = [stock]\n",
    "\n",
    "buy_sell_ridge_train = portfolio_return(returns_df, buy_dict_ridge_train, sell_dict_ridge_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_lasso_train = {}\n",
    "sell_dict_lasso_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_lasso):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "\n",
    "    if pred > 0:\n",
    "        if date in buy_dict_lasso_train:\n",
    "            buy_dict_lasso_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_lasso_train[date] = [stock]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_lasso_train:\n",
    "            sell_dict_lasso_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_lasso_train[date] = [stock]\n",
    "\n",
    "buy_sell_lasso_train = portfolio_return(returns_df, buy_dict_lasso_train, sell_dict_lasso_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_nn_train = {}\n",
    "sell_dict_nn_train = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(train_y_nn):\n",
    "    stock = training_df.iloc[idx]['Stock']\n",
    "    date = training_df.iloc[idx]['Date']\n",
    "\n",
    "    if pred > 0:\n",
    "        if date in buy_dict_nn_train:\n",
    "            buy_dict_nn_train[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_nn_train[date] = [stock]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_nn_train:\n",
    "            sell_dict_nn_train[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_nn_train[date] = [stock]\n",
    "\n",
    "buy_sell_nn_train = portfolio_return(returns_df, buy_dict_nn_train, sell_dict_nn_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buy-Sell based on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df[['Date', 'Stock', 'Return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test_df.columns\n",
    "list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.merge(test_df, merged_df[['Date', 'Stock', 'Return']], \n",
    "                      on=['Date', 'Return'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.drop(['bin_Return', 'Return'], axis=1, inplace=True)\n",
    "testing_df.reset_index(drop=True, inplace=True)\n",
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df =  pd.merge(train_df, merged_df[['Date', 'Stock', 'Return']], \n",
    "                      on=['Date', 'Return'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop(['bin_Return', 'Return'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree based on Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = best.predict_proba(X_test)\n",
    "#  Initialize buy and sell dictionaries\n",
    "buy_dict_dt = {}\n",
    "sell_dict_dt = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_gs):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    prob = probabilities[idx][1]  # Assuming class 1 is the positive class\n",
    "\n",
    "    if pred == 1:\n",
    "        if date in buy_dict_dt:\n",
    "            buy_dict_dt[date].append((stock, prob))\n",
    "        else:\n",
    "            buy_dict_dt[date] = [(stock, prob)]\n",
    "    else:\n",
    "        if date in sell_dict_dt:\n",
    "            sell_dict_dt[date].append((stock, prob))\n",
    "        else:\n",
    "            sell_dict_dt[date] = [(stock, prob)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by probability\n",
    "for date in buy_dict_dt:\n",
    "    buy_dict_dt[date] = sorted(buy_dict_dt[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_dt:\n",
    "    sell_dict_dt[date] = sorted(sell_dict_dt[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove probabilities, keeping only the stocks\n",
    "buy_dict_dt = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_dt.items()}\n",
    "sell_dict_dt = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_dt.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data = {\n",
    "    'buy_dict': buy_dict_dt,\n",
    "    'sell_dict': sell_dict_dt\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_decisiontree.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_dt = {}\n",
    "sell_dict_dt = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_gs):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_dt:\n",
    "            buy_dict_dt[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_dt[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_dt:\n",
    "            sell_dict_dt[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_dt[date] = [stock]\n",
    "\n",
    "buy_sell_dt = portfolio_return(returns_df, buy_dict_dt, sell_dict_dt, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = rf_classifier.predict_proba(X_test)\n",
    "#  Initialize buy and sell dictionaries\n",
    "buy_dict_rf = {}\n",
    "sell_dict_rf = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_rf):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    prob = probabilities[idx][1]  # Assuming class 1 is the positive class\n",
    "\n",
    "    if pred == 1:\n",
    "        if date in buy_dict_rf:\n",
    "            buy_dict_rf[date].append((stock, prob))\n",
    "        else:\n",
    "            buy_dict_rf[date] = [(stock, prob)]\n",
    "    else:\n",
    "        if date in sell_dict_rf:\n",
    "            sell_dict_rf[date].append((stock, prob))\n",
    "        else:\n",
    "            sell_dict_rf[date] = [(stock, prob)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by probability\n",
    "for date in buy_dict_rf:\n",
    "    buy_dict_rf[date] = sorted(buy_dict_rf[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_rf:\n",
    "    sell_dict_rf[date] = sorted(sell_dict_rf[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove probabilities, keeping only the stocks\n",
    "buy_dict_rf = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_rf.items()}\n",
    "sell_dict_rf = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_rf.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data = {\n",
    "    'buy_dict': buy_dict_rf,\n",
    "    'sell_dict': sell_dict_rf\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_randomforest.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_rf = {}\n",
    "sell_dict_rf = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_rf):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_rf:\n",
    "            buy_dict_rf[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_rf[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_rf:\n",
    "            sell_dict_rf[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_rf[date] = [stock]\n",
    "\n",
    "buy_sell_rf = portfolio_return(returns_df, buy_dict_rf, sell_dict_rf, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_ada = {}\n",
    "sell_dict_ada = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_ada):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_ada:\n",
    "            buy_dict_ada[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_ada[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_ada:\n",
    "            sell_dict_ada[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_ada[date] = [stock]\n",
    "\n",
    "buy_sell_ada = portfolio_return(returns_df, buy_dict_ada, sell_dict_ada, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = stacking_clf.predict_proba(X_test)\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_ada = {}\n",
    "sell_dict_ada = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_ada):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    prob = probabilities[idx][1]  # Assuming class 1 is the positive class\n",
    "\n",
    "    if pred == 1:\n",
    "        if date in buy_dict_ada:\n",
    "            buy_dict_ada[date].append((stock, prob))\n",
    "        else:\n",
    "            buy_dict_ada[date] = [(stock, prob)]\n",
    "    else:\n",
    "        if date in sell_dict_ada:\n",
    "            sell_dict_ada[date].append((stock, prob))\n",
    "        else:\n",
    "            sell_dict_ada[date] = [(stock, prob)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by probability\n",
    "for date in buy_dict_ada:\n",
    "    buy_dict_ada[date] = sorted(buy_dict_ada[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_ada:\n",
    "    sell_dict_ada[date] = sorted(sell_dict_ada[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove probabilities, keeping only the stocks\n",
    "buy_dict_ada = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_ada.items()}\n",
    "sell_dict_ada = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_ada.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data = {\n",
    "    'buy_dict': buy_dict_ada,\n",
    "    'sell_dict': sell_dict_ada\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_adaboost.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_log = {}\n",
    "sell_dict_log = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_log):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_log:\n",
    "            buy_dict_log[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_log[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_log:\n",
    "            sell_dict_log[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_log[date] = [stock]\n",
    "\n",
    "buy_sell_log = portfolio_return(returns_df, buy_dict_log, sell_dict_log, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = log_reg.predict_proba(X_test)\n",
    "#  Initialize buy and sell dictionaries\n",
    "buy_dict_log = {}\n",
    "sell_dict_log = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_log):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    prob = probabilities[idx][1]  # Assuming class 1 is the positive class\n",
    "\n",
    "    if pred == 1:\n",
    "        if date in buy_dict_log:\n",
    "            buy_dict_log[date].append((stock, prob))\n",
    "        else:\n",
    "            buy_dict_log[date] = [(stock, prob)]\n",
    "    else:\n",
    "        if date in sell_dict_log:\n",
    "            sell_dict_log[date].append((stock, prob))\n",
    "        else:\n",
    "            sell_dict_log[date] = [(stock, prob)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by probability\n",
    "for date in buy_dict_log:\n",
    "    buy_dict_log[date] = sorted(buy_dict_log[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_log:\n",
    "    sell_dict_log[date] = sorted(sell_dict_log[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove probabilities, keeping only the stocks\n",
    "buy_dict_log = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_log.items()}\n",
    "sell_dict_log = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_log.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data = {\n",
    "    'buy_dict': buy_dict_log,\n",
    "    'sell_dict': sell_dict_log\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_logistic.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_nn_clf = {}\n",
    "sell_dict_nn_clf = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_nn_clf):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred == 1:\n",
    "        if date in buy_dict_nn_clf:\n",
    "            buy_dict_nn_clf[date].append(stock)\n",
    "        else:\n",
    "            buy_dict_nn_clf[date] = [stock]\n",
    "    else:\n",
    "        if date in sell_dict_nn_clf:\n",
    "            sell_dict_nn_clf[date].append(stock)\n",
    "        else:\n",
    "            sell_dict_nn_clf[date] = [stock]\n",
    "\n",
    "buy_sell_nn_clf = portfolio_return(returns_df, buy_dict_nn_clf, sell_dict_nn_clf, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = best_mlp_clf.predict_proba(X_test)\n",
    "#  Initialize buy and sell dictionaries\n",
    "buy_dict_nn_clf = {}\n",
    "sell_dict_nn_clf = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_nn_clf):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    prob = probabilities[idx][1]  # Assuming class 1 is the positive class\n",
    "\n",
    "    if pred == 1:\n",
    "        if date in buy_dict_nn_clf:\n",
    "            buy_dict_nn_clf[date].append((stock, prob))\n",
    "        else:\n",
    "            buy_dict_nn_clf[date] = [(stock, prob)]\n",
    "    else:\n",
    "        if date in sell_dict_nn_clf:\n",
    "            sell_dict_nn_clf[date].append((stock, prob))\n",
    "        else:\n",
    "            sell_dict_nn_clf[date] = [(stock, prob)]\n",
    "\n",
    "\n",
    "# Sort the buy and sell dictionaries by probability\n",
    "for date in buy_dict_nn_clf:\n",
    "    buy_dict_nn_clf[date] = sorted(buy_dict_nn_clf[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_nn_clf:\n",
    "    sell_dict_nn_clf[date] = sorted(sell_dict_nn_clf[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove probabilities, keeping only the stocks\n",
    "buy_dict_nn_clf = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_nn_clf.items()}\n",
    "sell_dict_nn_clf = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_nn_clf.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data = {\n",
    "    'buy_dict': buy_dict_nn_clf,\n",
    "    'sell_dict': sell_dict_nn_clf\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_neuralnetwork_classifier.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cont models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn = {}\n",
    "sell_dict_regn = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_cont_pred):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date'].strftime('%Y-%m-%d')  # Keep datetime format as string\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_regn:\n",
    "            buy_dict_regn[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_regn[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_regn:\n",
    "            sell_dict_regn[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_regn[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_regn:\n",
    "    buy_dict_regn[date] = sorted(buy_dict_regn[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_regn:\n",
    "    sell_dict_regn[date] = sorted(sell_dict_regn[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_regn = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_regn.items()}\n",
    "sell_dict_regn = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_regn.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_regn = {\n",
    "    'buy_dict_regn': buy_dict_regn,\n",
    "    'sell_dict_regn': sell_dict_regn\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_regression.json', 'w') as f:\n",
    "    json.dump(output_data_regn, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to buy_sell_dict_regn.json\")\n",
    "\n",
    "buy_sell_regn = portfolio_return(returns_df, buy_dict_regn, sell_dict_regn, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_rf = {}\n",
    "sell_dict_regn_rf = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_rf_regn):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_regn_rf:\n",
    "            buy_dict_regn_rf[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_regn_rf[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_regn_rf:\n",
    "            sell_dict_regn_rf[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_regn_rf[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_regn_rf:\n",
    "    buy_dict_regn_rf[date] = sorted(buy_dict_regn_rf[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_regn_rf:\n",
    "    sell_dict_regn_rf[date] = sorted(sell_dict_regn_rf[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_regn_rf = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_regn_rf.items()}\n",
    "sell_dict_regn_rf = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_regn_rf.items()}\n",
    "\n",
    "# Convert the keys to strings for JSON serialization\n",
    "buy_dict_regn_rf_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in buy_dict_regn_rf.items()}\n",
    "sell_dict_regn_rf_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in sell_dict_regn_rf.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_regn_rf = {\n",
    "    'buy_dict_regn_rf': buy_dict_regn_rf_str,\n",
    "    'sell_dict_regn_rf': sell_dict_regn_rf_str\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_randomforestregression.json', 'w') as f:\n",
    "    json.dump(output_data_regn_rf, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to nse_monthly_rfregn.json\")\n",
    "\n",
    "buy_sell_regn_rf = portfolio_return(returns_df, buy_dict_regn_rf, sell_dict_regn_rf, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_cart = {}\n",
    "sell_dict_regn_cart = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_dt_regn):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_regn_cart:\n",
    "            buy_dict_regn_cart[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_regn_cart[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_regn_cart:\n",
    "            sell_dict_regn_cart[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_regn_cart[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_regn_cart:\n",
    "    buy_dict_regn_cart[date] = sorted(buy_dict_regn_cart[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_regn_cart:\n",
    "    sell_dict_regn_cart[date] = sorted(sell_dict_regn_cart[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_regn_cart = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_regn_cart.items()}\n",
    "sell_dict_regn_cart = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_regn_cart.items()}\n",
    "\n",
    "# Convert the keys to strings for JSON serialization\n",
    "buy_dict_regn_cart_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in buy_dict_regn_cart.items()}\n",
    "sell_dict_regn_cart_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in sell_dict_regn_cart.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_regn_cart = {\n",
    "    'buy_dict_regn_cart': buy_dict_regn_cart_str,\n",
    "    'sell_dict_regn_cart': sell_dict_regn_cart_str\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_cart.json', 'w') as f:\n",
    "    json.dump(output_data_regn_cart, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to nse_weekly_regn_cart.json\")\n",
    "\n",
    "buy_sell_regn_cart = portfolio_return(returns_df, buy_dict_regn_cart, sell_dict_regn_cart, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_lasso = {}\n",
    "sell_dict_lasso = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_lasso_pred):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_lasso:\n",
    "            buy_dict_lasso[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_lasso[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_lasso:\n",
    "            sell_dict_lasso[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_lasso[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_lasso:\n",
    "    buy_dict_lasso[date] = sorted(buy_dict_lasso[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_lasso:\n",
    "    sell_dict_lasso[date] = sorted(sell_dict_lasso[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_lasso = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_lasso.items()}\n",
    "sell_dict_lasso = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_lasso.items()}\n",
    "\n",
    "# Convert the keys to strings for JSON serialization\n",
    "buy_dict_lasso_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in buy_dict_lasso.items()}\n",
    "sell_dict_lasso_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in sell_dict_lasso.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_lasso = {\n",
    "    'buy_dict_lasso': buy_dict_lasso_str,\n",
    "    'sell_dict_lasso': sell_dict_lasso_str\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_lassoregression.json', 'w') as f:\n",
    "    json.dump(output_data_lasso, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to nse_monthly_lasso.json\")\n",
    "\n",
    "\n",
    "buy_sell_lasso = portfolio_return(returns_df, buy_dict_lasso, sell_dict_lasso,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_ridge = {}\n",
    "sell_dict_regn_ridge = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_ridge_pred):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_regn_ridge:\n",
    "            buy_dict_regn_ridge[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_regn_ridge[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_regn_ridge:\n",
    "            sell_dict_regn_ridge[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_regn_ridge[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_regn_ridge:\n",
    "    buy_dict_regn_ridge[date] = sorted(buy_dict_regn_ridge[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_regn_ridge:\n",
    "    sell_dict_regn_ridge[date] = sorted(sell_dict_regn_ridge[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_regn_ridge = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_regn_ridge.items()}\n",
    "sell_dict_regn_ridge = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_regn_ridge.items()}\n",
    "\n",
    "# Convert the keys to strings for JSON serialization\n",
    "buy_dict_regn_ridge_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in buy_dict_regn_ridge.items()}\n",
    "sell_dict_regn_ridge_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in sell_dict_regn_ridge.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_regn_ridge = {\n",
    "    'buy_dict_regn_ridge': buy_dict_regn_ridge_str,\n",
    "    'sell_dict_regn_ridge': sell_dict_regn_ridge_str\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_ridgeregression.json', 'w') as f:\n",
    "    json.dump(output_data_regn_ridge, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to nse_monthly_regn_ridge.json\")\n",
    "\n",
    "# Calculate the portfolio return\n",
    "buy_sell_ridge = portfolio_return(returns_df, buy_dict_regn_ridge, sell_dict_regn_ridge, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize buy and sell dictionaries\n",
    "buy_dict_regn_nn = {}\n",
    "sell_dict_regn_nn = {}\n",
    "\n",
    "# Classify stocks into buy and sell based on predictions\n",
    "for idx, pred in enumerate(y_pred_nn):\n",
    "    stock = testing_df.iloc[idx]['Stock']\n",
    "    date = testing_df.iloc[idx]['Date']\n",
    "    \n",
    "    if pred > 0:\n",
    "        if date in buy_dict_regn_nn:\n",
    "            buy_dict_regn_nn[date].append((stock, pred))\n",
    "        else:\n",
    "            buy_dict_regn_nn[date] = [(stock, pred)]\n",
    "    elif pred < 0:\n",
    "        if date in sell_dict_regn_nn:\n",
    "            sell_dict_regn_nn[date].append((stock, pred))\n",
    "        else:\n",
    "            sell_dict_regn_nn[date] = [(stock, pred)]\n",
    "\n",
    "# Sort the buy and sell dictionaries by predicted value\n",
    "for date in buy_dict_regn_nn:\n",
    "    buy_dict_regn_nn[date] = sorted(buy_dict_regn_nn[date], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for date in sell_dict_regn_nn:\n",
    "    sell_dict_regn_nn[date] = sorted(sell_dict_regn_nn[date], key=lambda x: x[1])\n",
    "\n",
    "# Remove predicted values, keeping only the stocks\n",
    "buy_dict_regn_nn = {date: [stock for stock, _ in stocks] for date, stocks in buy_dict_regn_nn.items()}\n",
    "sell_dict_regn_nn = {date: [stock for stock, _ in stocks] for date, stocks in sell_dict_regn_nn.items()}\n",
    "\n",
    "# Convert the keys to strings for JSON serialization\n",
    "buy_dict_regn_nn_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in buy_dict_regn_nn.items()}\n",
    "sell_dict_regn_nn_str = {date.strftime('%Y-%m-%d'): stocks for date, stocks in sell_dict_regn_nn.items()}\n",
    "\n",
    "# Save the dictionaries to a local JSON file\n",
    "output_data_regn_nn = {\n",
    "    'buy_dict_regn_nn': buy_dict_regn_nn_str,\n",
    "    'sell_dict_regn_nn': sell_dict_regn_nn_str\n",
    "}\n",
    "\n",
    "with open('C:/Users/satya/OneDrive/Desktop/model_outputs/nse_weekly_regn_nn.json', 'w') as f:\n",
    "    json.dump(output_data_regn_nn, f, indent=4)\n",
    "\n",
    "print(\"Dictionaries saved to nse_weekly_neuralnetwork.json\")\n",
    "\n",
    "# Calculate the portfolio return\n",
    "buy_sell_nn = portfolio_return(returns_df, buy_dict_regn_nn, sell_dict_regn_nn, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_dt_train.items())),orient='index', columns=['Decision Tree'])\n",
    "rf_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_rf_train.items())),orient='index', columns=['RandomForest'])\n",
    "ada_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_ada_train.items())),orient='index', columns=['AdaBoost'])\n",
    "regn_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_regn_train.items())),orient='index', columns=['Regn'])\n",
    "log_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_log_train.items())),orient='index', columns=['Logistic Regn'])\n",
    "nn_clf_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_nn_clf_train.items())),orient='index', columns=['NN_Clf'])\n",
    "ridge_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_ridge_train.items())),orient='index', columns=['Ridge'])\n",
    "lasso_df_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_lasso_train.items())),orient='index', columns=['Lasso'])\n",
    "rf_regn_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_regn_rf_train.items())),orient='index', columns=['RFRegn'])\n",
    "cart_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_cart_train.items())),orient='index', columns=['CART'])\n",
    "nn_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_nn_train.items())),orient='index', columns=['NN'])\n",
    "svr_train = pd.DataFrame.from_dict(dict(sorted(buy_sell_svr_train.items())),orient='index', columns=['SVR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_train = pd.concat([dt_df_train, rf_df_train, log_df_train, nn_clf_train, ada_df_train,  \n",
    "                        regn_df_train, rf_regn_train, cart_train, ridge_df_train, \n",
    "                        lasso_df_train, nn_train], axis=1)\n",
    "# svr_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "def calculate_cumulative_returns(df):\n",
    "    cumulative_returns_df = (1 + df / 100).cumprod()\n",
    "    return cumulative_returns_df\n",
    "\n",
    "def plot_strategy_performance(strategy_df):\n",
    "    # Ensure the index is a datetime index\n",
    "    strategy_df.index = pd.to_datetime(strategy_df.index)\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns_df = calculate_cumulative_returns(strategy_df)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each strategy\n",
    "    for strategy in cumulative_returns_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cumulative_returns_df.index,\n",
    "            y=cumulative_returns_df[strategy],\n",
    "            mode='lines',\n",
    "            name=strategy\n",
    "        ))\n",
    "    \n",
    "    # Add titles and labels\n",
    "    fig.update_layout(\n",
    "        title='Cumulative Returns of Strategies',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Cumulative Return',\n",
    "        legend_title='Strategies'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "plot_strategy_performance(plot_df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots on testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_sell_regn = {pd.to_datetime((date)): stocks for date, stocks in buy_sell_regn.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_dt.items())),orient='index', columns=['Decision Tree'])\n",
    "rf_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_rf.items())),orient='index', columns=['RandomForest'])\n",
    "ada_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_ada.items())),orient='index', columns=['AdaBoost'])\n",
    "regn_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_regn.items())),orient='index', columns=['Regn'])\n",
    "log_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_log.items())),orient='index', columns=['Logistic Regn'])\n",
    "nn_clf = pd.DataFrame.from_dict(dict(sorted(buy_sell_nn_clf.items())),orient='index', columns=['NN_Clf'])\n",
    "ridge_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_ridge.items())),orient='index', columns=['Ridge'])\n",
    "lasso_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_lasso.items())),orient='index', columns=['Lasso'])\n",
    "rf_regn = pd.DataFrame.from_dict(dict(sorted(buy_sell_regn_rf.items())),orient='index', columns=['RFRegn'])\n",
    "cart = pd.DataFrame.from_dict(dict(sorted(buy_sell_regn_cart.items())),orient='index', columns=['Cart'])\n",
    "nn_df = pd.DataFrame.from_dict(dict(sorted(buy_sell_nn.items())),orient='index', columns=['NN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat([dt_df, rf_df,  log_df, nn_clf, ada_df, \n",
    "                    regn_df, ridge_df,lasso_df,  rf_regn, cart, nn_df], axis=1)\n",
    "# svr_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "def calculate_cumulative_returns(df):\n",
    "    cumulative_returns_df = (1 + df / 100).cumprod()\n",
    "    return cumulative_returns_df\n",
    "\n",
    "def plot_all_strategy_performance(strategy_df):\n",
    "    # Ensure the index is a datetime index\n",
    "    strategy_df.index = pd.to_datetime(strategy_df.index)\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns_df = calculate_cumulative_returns(strategy_df)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each strategy\n",
    "    for strategy in cumulative_returns_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cumulative_returns_df.index,\n",
    "            y=cumulative_returns_df[strategy],\n",
    "            mode='lines',\n",
    "            name=strategy\n",
    "        ))\n",
    "    \n",
    "    # Add titles and labels\n",
    "    fig.update_layout(\n",
    "        title='Cumulative Returns of Strategies',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Cumulative Return',\n",
    "        legend_title='Strategies'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "plot_all_strategy_performance(plot_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse = yf.download('^CRSLDX', '2005-01-01')\n",
    "nse = nse['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_nse = nse.resample('W').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_profit = pd.DataFrame(weekly_nse.pct_change()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_test = nse_profit[(nse_profit.index > '2019') ]\n",
    "nse_test = nse_test[(nse_test.index < '2024') ]\n",
    "nse_train = nse_profit[nse_profit.index < '2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_cagrs(nse_train, 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_sharpe_ratios(nse_train, 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_cagrs(nse_test, 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_sharpe_ratios(nse_test, 'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vol Adjusted graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([plot_df_train, nse_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:-2]\n",
    "# Calculate the volatility (standard deviation) of each strategy and the benchmark\n",
    "volatilities = x_train.std()\n",
    "\n",
    "# Ex_traintract the volatility of the benchmark\n",
    "benchmark_volatility = volatilities['Adj Close']\n",
    "\n",
    "# Calculate the scaling factor for each strategy to match the benchmark volatility\n",
    "scaling_factors = benchmark_volatility / volatilities\n",
    "\n",
    "# Apply the scaling factor to each strategy\n",
    "adjusted_x_train = x_train.mul(scaling_factors, axis=1)\n",
    "\n",
    "# Calculate the cumulative returns\n",
    "cumulative_returns = (1 + adjusted_x_train/100).cumprod()\n",
    "\n",
    "\n",
    "# Generate individual plots for each strategy\n",
    "for column in cumulative_returns.columns:\n",
    "    if column != 'Adj Close':\n",
    "        # Create Plotly traces\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add the benchmark trace\n",
    "        fig.add_trace(go.Scatter(x=cumulative_returns.index, y=cumulative_returns['Adj Close'], mode='lines', name='Benchmark'))\n",
    "\n",
    "        # Add the adjusted strategy trace\n",
    "        fig.add_trace(go.Scatter(x=cumulative_returns.index, y=cumulative_returns[column], mode='lines', name=f'{column} (Adjusted)'))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Benchmark vs {column} (Adjusted)',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Cumulative Return',\n",
    "            legend_title='Legend',\n",
    "            template='plotly_dark'\n",
    "        )\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "vol_adj_plot(x_train[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:-2]\n",
    "# Calculate the volatility (standard deviation) of each strategy and the benchmark\n",
    "volatilities = x_train.std()\n",
    "\n",
    "# Ex_traintract the volatility of the benchmark\n",
    "benchmark_volatility = volatilities['Adj Close']\n",
    "\n",
    "# Calculate the scaling factor for each strategy to match the benchmark volatility\n",
    "scaling_factors = benchmark_volatility / volatilities\n",
    "\n",
    "# Apply the scaling factor to each strategy\n",
    "adjusted_x_train = x_train.mul(scaling_factors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "def calculate_cumulative_returns(df):\n",
    "    cumulative_returns_df = (1 + df / 100).cumprod()\n",
    "    return cumulative_returns_df\n",
    "\n",
    "def plot_strategy_performance(strategy_df):\n",
    "    # Ensure the index is a datetime index\n",
    "    strategy_df.index = pd.to_datetime(strategy_df.index)\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns_df = calculate_cumulative_returns(strategy_df)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each strategy\n",
    "    for strategy in cumulative_returns_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cumulative_returns_df.index,\n",
    "            y=cumulative_returns_df[strategy],\n",
    "            mode='lines',\n",
    "            name=strategy\n",
    "        ))\n",
    "    \n",
    "    # Add titles and labels\n",
    "    fig.update_layout(\n",
    "        title='Cumulative Returns of Strategies',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Cumulative Return',\n",
    "        legend_title='Strategies'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "plot_strategy_performance(adjusted_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagr_adj = calculate_overall_cagrs(adjusted_x_train, 'W')\n",
    "cagr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_adj = calculate_overall_sharpe_ratios(adjusted_x_train, 'W')\n",
    "sharpe_ratio_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.concat([plot_df, nse_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[:-2]\n",
    "# Calculate the volatility (standard deviation) of each strategy and the benchmark\n",
    "volatilities = x_test.std()\n",
    "\n",
    "# Ex_testtract the volatility of the benchmark\n",
    "benchmark_volatility = volatilities['Adj Close']\n",
    "\n",
    "# Calculate the scaling factor for each strategy to match the benchmark volatility\n",
    "scaling_factors = benchmark_volatility / volatilities\n",
    "\n",
    "# Apply the scaling factor to each strategy\n",
    "adjusted_x_test = x_test.mul(scaling_factors, axis=1)\n",
    "\n",
    "# Calculate the cumulative returns\n",
    "cumulative_returns = (1 + adjusted_x_test/100).cumprod()\n",
    "\n",
    "\n",
    "# Generate individual plots for each strategy\n",
    "for column in cumulative_returns.columns:\n",
    "    if column != 'Adj Close':\n",
    "        # Create Plotly traces\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add the benchmark trace\n",
    "        fig.add_trace(go.Scatter(x=cumulative_returns.index, y=cumulative_returns['Adj Close'], mode='lines', name='Benchmark'))\n",
    "\n",
    "        # Add the adjusted strategy trace\n",
    "        fig.add_trace(go.Scatter(x=cumulative_returns.index, y=cumulative_returns[column], mode='lines', name=f'{column} (Adjusted)'))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Benchmark vs {column} (Adjusted)',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Cumulative Return',\n",
    "            legend_title='Legend',\n",
    "            template='plotly_dark'\n",
    "        )\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagr_adj = calculate_overall_cagrs(adjusted_x_test, 'W')\n",
    "cagr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_adj = calculate_overall_sharpe_ratios(adjusted_x_test, 'W')\n",
    "type(sharpe_ratio_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'Adj Close' entry from the series and sorting by value\n",
    "sr_adj_filtered = sharpe_ratio_adj.drop(['Adj Close', 'Decision Tree']).sort_values()\n",
    "\n",
    "# Creating the horizontal bar chart\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(sr_adj_filtered.index, sr_adj_filtered.values, color='skyblue')\n",
    "\n",
    "# Adding the values at the end of the bars\n",
    "for index, value in enumerate(sr_adj_filtered.values):\n",
    "    plt.text(value, index, f'{value:.2f}', va='center')\n",
    "\n",
    "# Adding the dotted line for 'Adj Close' value\n",
    "plt.axvline(x=sharpe_ratio_adj['Adj Close'], color='red', linestyle='--')\n",
    "\n",
    "# Removing the top and right spines (axes)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Turning off gridlines\n",
    "plt.grid(False)\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel('Sharpe Ratio')\n",
    "plt.ylabel('Models')\n",
    "plt.title('Sharpe Ratio of Different Models')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns_df = calculate_cumulative_returns(adjusted_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns_df_train = calculate_cumulative_returns(adjusted_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columns = ['RandomForest',  'Logistic Regn', 'NN_Clf']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot each column against 'Adj Close'\n",
    "ax.plot(cumulative_returns_df.index, cumulative_returns_df['Adj Close'], label='Benchmark', color='blue')\n",
    "\n",
    "for column in columns:\n",
    "    ax.plot(cumulative_returns_df.index, cumulative_returns_df[column], label=column)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Classification Models')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = ['NN', 'CART', 'RFRegn', 'Ridge', 'Regn']\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot each column against 'Adj Close'\n",
    "ax.plot(cumulative_returns_df_train.index, cumulative_returns_df_train['Adj Close'], label='Benchmark', color='blue')\n",
    "\n",
    "for column in cols:\n",
    "    ax.plot(cumulative_returns_df_train.index, cumulative_returns_df_train[column], label=column)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Regression Models')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_corr = plot_df.corr()\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "sns.heatmap(return_corr, annot=True, cmap='coolwarm', vmin=-0.5, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
